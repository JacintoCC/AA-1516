---
title: "Trabajo 3 Aprendizaje Automático"
author: "Jacinto Carrasco Castillo"
date: "1 de junio de 2016"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
---

Establecemos la semilla para poder reproducir los resultados

```{r}
set.seed(3141592)
```

  Defino la función que pasa un hiperplano a una función usada en prácticas anteriorores:
```{r}
# Obtención de una función que recibe un vector como entrada
# a partir de un hiperplano y devuelve el producto escalar con los
# coeficientes del hiperplano.
# @param vec Vector de coeficientes del hiperplano
# @param change_signs Valor lógico que determina si debemos cambiar
#                     el signo de los coeficientes del vector
# @return función que realiza el producto escalar de un vector con 
#         los coeficientes del hiperplano
hypplane_to_func <- function( vec, change_signs = FALSE ){
  if(change_signs){
    vec[2:length(vec)] <- -vec[2:length(vec)]
  }
  f <- function(x){
    # @return( vec[1]*x[1]+vec[2]*x[2]+vec[3]*x[3] )
    return( x %*% vec )
  }
  return(f)
}
```

# Ejercicio 1

**Usar el conjunto de datos Auto que es parte del paquete ISLR.**
**En este ejercicio desarrollaremos un modelo para predecir si un coche tiene un consumo de carburante alto o bajo usando la base de datos Auto. Se considerará bajo cuando sea superior a la mediana de la variable *mpg* y alto en caso contrario.**

Cargamos en primer lugar la librería $\texttt{ISLR}$ que hemos instalado previamente y cargamos la base de datos $\texttt{Auto}$

```{r, message=FALSE, warning=FALSE}
library(ISLR)
data("Auto")
```


### a) Usar las funciones de R $\texttt{pairs()}$ y $\texttt{boxplot()}$ para investigar la dependencia entre *mpg* y las otras características. ¿Cuáles de las otras características parece más útil para predecir *mpg*? Justificar la respuesta.

Mostramos con la función $\texttt{pairs}$ las gráficas de todas las combinaciones posibles de variables.

```{r}
pairs(Auto)
```

Las características que parecen más útiles a simple vista para la predicción de *mpg* son *displacement*, *horsepower* y *weight*. Para las tres características la relación es inversamente proporcional; se aprecia menor *mpg*, es decir, mayor consumo, conforme crece *displacement*, la potencia del coche y el peso. Para las demás categorías, también se ve correlación con el año o el número de cilindros.  

Para usar el método $\texttt{boxplot}$ y decidir qué variables son las más relevantes, esperaremos a haber clasificado los datos según su consumo sea alto o bajo. Ahora podríamos aplicar $\texttt{boxplot}$ a *mpg* con respecto a *cylinders*, por ejemplo:

```{r}
boxplot(Auto$mpg ~ Auto$cylinders, main = "Boxplot MPG ~ cylinders",
        xlab = "cylinders", ylab = "MPG")
```

Se observa mucho solapamiento de todos las clases de cilindros en torno al 20 de los *mpg*, con lo que se puede intuir que no será muy eficaz. Además, para 4 cilindros se observa una variabilidad muy alta, con lo que habrá una cantidad importante de datos con etiquetas distintas.


### b) Seleccionar las variables predictoras que considere más relevantes.

Atendiendo a las gráficas que hemos visto en el apartado anterior, las variables más relevantes serán *displacement*, *cylinders*, *horsepower* y *weight*.

### c) Particionar el conjunto de datos en un conjunto de entrenamiento ($80\%$) y otro de test ($20\%$). Justificar el procedimiento usado.

Obtenemos el número de datos en la BD y se toma una muestra de entre los índices de tamaño el $80\%$ del total. Esto nos permite obtener fácilmente la partición de entrenamiento y la de test.

```{r}
num.datos <- length(Auto)
train.index.Auto <- sample(1:num.datos, size = num.datos*0.8, 
                           replace = F)

train.Auto <- Auto[train.index.Auto,]
test.Auto <- Auto[-train.index.Auto,]
```

### d) Crear una variable binaria, mpg01, que será igual 1 si la variable mpg contiene un valor por encima de la mediana, y -1 si mpg contiene un valor por debajo de la mediana. La mediana se puede calcular usando la función median(). (Nota: puede resultar útil usar la función data.frames() para unir en un mismo conjunto de datos la nueva variable mpg01 y las otras variables de Auto).

Para crear una variable *mpg01* aplicamos directamente la función $\texttt{sign}$. Entonces sustituimos en el $\texttt{dataframe}$ la columna *mpg* por la *mpg01*

```{r}
mpg01 <- sign(Auto$mpg - median(Auto$mpg))
Auto <- data.frame(cbind(mpg01,Auto[-1]))
```

  Repetimos ahora el análisis con $\texttt{boxplot}$ de las variables seleccionadas:
  
```{r}
par(mfrow=c(2,2))
boxplot(Auto$mpg01 ~ Auto$cylinders, main = "Boxplot MPG ~ cylinders",
        xlab = "cylinders", ylab = "MPG01")
boxplot(Auto$displacement ~ Auto$mpg01, 
        main = "Boxplot displacement ~  MPG01",
        ylab = "displacement", xlab = "MPG01")
boxplot(Auto$weight ~ Auto$mpg01, 
        main = "Boxplot weight ~  MPG01",
        ylab = "weight", xlab = "MPG01")
boxplot(Auto$horsepower ~ Auto$mpg01, 
        main = "Boxplot horsepower ~  MPG01",
        ylab = "horsepower", xlab = "MPG01")
par(mfrow=c(1,1))
```

  Se observa que siempre existe solapamiento en los diferentes *boxplot* para la clase 1 y la clase -1, aunque para estas variables el solapamiento es inferior al de otras, por ejemplo para la aceleración:
  
```{r}
boxplot( Auto$acceleration ~ Auto$mpg01, 
        main = "Boxplot Aceleración ~  MPG01",
        ylab = "Aceleración", xlab = "MPG01")
```

  +. Ajustar un modelo de regresión logística a los datos de entrenamiento y predecir *mpg01* usando las variables seleccionadas en b). ¿Cuál es el error de test del modelo? Justificar la respuesta.

  Ahora podemos hacer una partición de los datos de manera que estén repartidas las categorías en la partición de entrenamiento y la de test:
  
```{r}
high_mpg <- Auto[Auto$mpg01==1,
                 c("mpg01","displacement", "cylinders", 
                   "horsepower", "weight")]
low_mpg <- Auto[Auto$mpg01==-1,
                c("mpg01","displacement", "cylinders", 
                  "horsepower", "weight")]

train.index.h <- sample(1:nrow(high_mpg), size = 0.8*nrow(high_mpg),
                        replace = F)
train.index.l <- sample(1:nrow(low_mpg), size = 0.8*nrow(low_mpg),
                        replace = F)

train.Auto <- rbind(high_mpg[train.index.h,],
                    low_mpg[train.index.l,])
test.Auto <- rbind(high_mpg[-train.index.h,],
                    low_mpg[-train.index.l,])
```

Sin embargo, cuando ajustamos el modelo a los datos de entrenamiento, no conocemos toda la muestra. Por tanto, ajustamos las etiquetas de los datos de entrenamiento según la mediana de los mismos datos de entrenamiento. Hacemos lo mismo para los datos de test.

```{r}
train.Auto[,1] <- sign(train.Auto[,1]-median(train.Auto[,1]))
test.Auto[,1] <- sign(test.Auto[,1]-median(test.Auto[,1]))
```


  Cargamos las bibliotecas requeridas.
  
```{r, message=FALSE, warning=FALSE}
library(e1071)
library(ROCR)
```

Aplicamos la función $\texttt{glm}$, obteniendo un modelo.

```{r}
glm.model <- glm(as.factor(mpg01) ~ horsepower + weight + 
                   displacement + cylinders,
                 data = Auto, family = "binomial")
coefs <- coef(glm.model)
```

  Con $\texttt{predict}$ podemos obtener, para los datos de test, bien la probabilidad de pertenecer a una clase si le pasamos el argumento $\texttt{type=response}$ o bien el valor esperado (con lo que podemos obtener la clase a la que sería asignado cada valor aplicándole la función signo). La primera de las opciones nos permite ajustar el nivel de probabilidad a partir del cual nos decantamos por una de las opciones. Esto sería interesante si una de las clases conllevara un mayor riesgo equivocarnos o tuviéramos conocimiento sobre la distribución de los datos. Asignaremos la clase 1 si la probabilidad es mayor que $0.5$, pues es lo habitual y en este caso no cuentan más unos errores que otros.

```{r}
prob.1.glm <- predict.glm(glm.model, newdata = test.Auto, 
                          type = "response")
pred.label.glm <- ifelse(prob.1.glm > 0.5, 1, -1)
prediction.glm <- prediction(prob.1.glm,test.Auto[,1])
E.test <- sum(pred.label.glm != test.Auto["mpg01"])/nrow(test.Auto)*100
cat("Error de test para regresión logística: ", E.test)
```
  
  El error de test es de un $5\%$, que es una buena medida teniendo en cuenta los datos de los que partimos y que no se ha realizado un estudio en profundidad sobre las variables seleccionadas.
  
  +. Ajustar un modelo K-NN a los datos de entrenamiento y predecir *mpg01* usando solamente las variables seleccionadas en b). ¿Cuál es el error de test en el modelo? ¿Cuál es el valor de $K$ que mejor ajusta los datos? Justificar la respuesta. (Usar el paquete class de R)

  Cargamos la biblioteca *class* para usar $\texttt{knn}$.
  
```{r, message=FALSE, warning=FALSE}
library(class)
```

  Comenzamos escalando los datos para que los rangos de los atributon no influyan:
  
```{r}
scale.factor <- scale(train.Auto[-1])
train.Auto[-1] <- scale.factor
test.Auto[-1] <- scale(test.Auto[-1], 
                       center = attr(scale.factor,"scaled:center"),
                       scale = attr(scale.factor,"scaled:scale"))
```


  Antes de pasar a ajustar el parámetro $k$, vemos el resultado de realizar $\texttt{1-NN}$. Tomamos las etiquetas dadas y la probabilidad de asignarle 1 a un dato (el atributo *prob* del modelo devuelto por $\texttt{knn}$ devuelve la probabilidad de asignación a la clase dada, con lo que tenemos que cambiarla).
  
```{r}
knn.model <- knn(train.Auto[,-1], test.Auto[,-1], train.Auto[,1],
                 k=1, prob = TRUE)
pred.label.knn <- sign(as.numeric(knn.model)-1.5)
prob.1.knn <- sapply(1:length(knn.model), function(i)
  if(pred.label.knn[i]==1)  return(attr(knn.model,"prob")[i])
  else return(1- attr(knn.model,"prob")[i]))
E.test <-sum(test.Auto[,"mpg01"]!=pred.label.knn)/nrow(test.Auto)*100
cat("Error de test para 1-NN: ", E.test)
```

Para $k=1$ se obtiene un error considerablemente mayor que para $\texttt{glm}$. Veremos cuál es el resultado ajustando un número de vecinos a considerar en la votación:

```{r}
tuned.knn <- tune.knn(as.matrix(train.Auto[,-1]),
                      as.factor(train.Auto[,1]), 
                      k=1:10,
                      tunecontrol = tune.control(sampling = "cross"))
best.k <- tuned.knn$best.parameters[1]
best.model.knn <- knn(train.Auto[,-1], test.Auto[,-1], 
                      train.Auto[,1], k=best.k, prob = TRUE)

pred.label.knn <- sign(as.numeric(best.model.knn)-1.5)
prob.1.knn <- sapply(1:nrow(test.Auto), function(i)
  if(pred.label.knn[i]==1)  return(attr(best.model.knn,"prob")[i])
  else return(1- attr(best.model.knn,"prob")[i]))

prediction.knn <- prediction(prob.1.knn,test.Auto[,1])
E.test <- sum(test.Auto[,1] != pred.label.knn)/nrow(test.Auto)*100
cat("Error en test", paste(best.k,"-NN: ",sep=""), E.test)
```

  Hemos reducido el error ajustando el parámetro $k$ hasta un $10\%$, con lo que nos acercamos al dato obtenido por $\texttt{glm}$. Para realizar una mejor comparación, compararemos las curvas ROC.

  +. Pintar las curvas ROC (instalar paquete $\texttt{ROCR}$ en R) y comparar y valorar los resultados obtenidos para ambos modelos.
  
```{r}
perf.glm <- performance(prediction.glm,"tpr","fpr")
auc.glm <- performance(prediction.glm,"auc")@y.values[[1]]
plot(perf.glm@x.values[[1]],perf.glm@y.values[[1]],type='l', 
     main = paste('Área Bajo la Curva GLM:', auc.glm), 
     xlab='Tasa falsos positivos',
     ylab = 'Tasa verdaderos positivos')
segments(0,0,1,1, col=3)
```

```{r}
perf.knn <- performance(prediction.knn,"tpr","fpr")
auc.knn <- performance(prediction.knn,"auc")@y.values[[1]]
plot(perf.knn@x.values[[1]],perf.knn@y.values[[1]],type='l', 
     main = paste('Área Bajo la Curva KNN:', auc.knn),
     xlab='Tasa falsos positivos',
     ylab = 'Tasa verdaderos positivos')
segments(0,0,1,1, col=3) 
```

 Pintamos ambas gráficas superpuestas para compararlas

```{r}
plot(perf.glm@x.values[[1]],perf.glm@y.values[[1]],type='l', 
     main = 'Curvas ROC GLM y KNN', xlab='Tasa falsos positivos',
     ylab = 'Tasa verdaderos positivos')
points(perf.knn@x.values[[1]],perf.knn@y.values[[1]],type='l', col=4)
segments(0,0,1,1, col=3)
```

  Las curvas y las áreas debajo de las curvas nos dan resultados distintos a los de los errores de test obtenidos (no es significativo puesto que sólo se ha obtenido un error de test), con mejores resultados para KNN.

### e) Bonus 1. Estimar el error de test de ambos modelos (RL, K-NN) pero usando validación cruzada de 5-particiones. Comparar con los resultados obtenidos en el punto anterior. 

### f) Bonus 2. Ajustar el mejor modelo de regresión posible considerando la variable *mpg* como salida y el resto como predictoras. Justificar el modelo ajustado en base al patrón de los residuos. Estimar su error de entrenamiento y test.



```{r}
detach("package:ROCR")
detach("package:e1071")
detach("package:ISLR")
detach("package:class")
```

# Ejercicio 2
## Usar la base de datos *Boston* (en el paquete $\texttt{MASS}$ de $\texttt{R}$) para ajustar un modelo que prediga si dado un suburbio este tiene una tasa de criminalidad (*crim*) por encima o por debajo de la mediana. Para ello considere la variable *crim* como la variable salida y el resto como variables predictoras.

En primer lugar se establece de nuevo la semilla para poder realizar un análisis sin ejecutar todo el código previo.
```{r}
set.seed(3141592)
```


```{r, message=FALSE, warning=FALSE}
library(MASS)
library(glmnet)
```

```{r}
data("Boston")
```

#### a) Encontrar el subconjunto óptimo de variables predictoras a partir de un modelo de regresión-LASSO (usar paquete *glmnet* de R) donde seleccionamos sólo aquellas variables con coeficiente mayor de un umbral prefijado.

Comenzamos generando las matrices que nos permitirán un más cómodo manejo de los datos para las funciones a utilizar en este apartado.

```{r}
x <- model.matrix(crim~., Boston)[,-1]
y <- Boston$crim
```


Obtenemos el valor óptimo del parámetro $\lambda$ haciendo validación cruzada. Puesto que pretendemos obtener un modelo de regresión, usamos la familia gaussiana de funciones:

```{r}
cv.lasso <- cv.glmnet(x, y, family = "gaussian")

lambda.min <- cv.lasso$lambda.min
cat("Lambda con menor error: ", lambda.min)
```

Ahora, separamos los datos en *train* y *test* de la misma manera que en el ejercicio anterior y, fijado $\lambda$, fijamos un umbral a partir del cual considerar relevantes las variables.

```{r}
train.index <- sample(1:nrow(x), size = nrow(x)*4/5)

x.train.boston <- x[train.index,]
y.train.boston <- y[train.index]

x.test.boston <- x[-train.index,]
y.test.boston <- y[-train.index]
```

```{r}
threshold <- 0.5
lasso.model <- glmnet(x.train.boston, y.train.boston, 
                      lambda = lambda.min)
lasso.coef <- predict(lasso.model, type = "coeff", s=lambda.min)[1:14,]
selected.var <- names(lasso.coef[abs(lasso.coef) > threshold])[-1]
```

#### b) Ajustar un modelo de regresión regularizada con “weight-decay” (ridge-regression) y las variables seleccionadas. Estimar el error residual del modelo y discutir si el comportamiento de los residuos muestran algún indicio de “underfitting”.

Ahora, una vez seleccionadas las variables a considerar (*chas*, *nox*, *dis*, *rad*), estimamos de nuevo el mejor $\lambda$ para el modelo *weight decay*:

```{r}
x <- model.matrix(crim~., Boston[c("crim",selected.var)])[,-1]

x.train.boston <- x[train.index,]
x.test.boston <- x[-train.index,]

cv.wdecay <- cv.glmnet(x.train.boston, y.train.boston, 
                       family = "gaussian", alpha=0) 

lambda.min <- cv.wdecay$lambda.min 
cat("Lambda con menor error: ", lambda.min)
```

Fijado $\lambda$ y las variables seleccionadas, vemos su error cuadrático medio: 

```{r}
ridge.mod = glmnet(x, y, alpha=0, lambda=lambda.min)
ridge.pred = predict (ridge.mod, s=lambda.min, newx=x.test.boston)
cat("MSE óptimo", mean((ridge.pred - y.test.boston)^2))
```

Para ver si hay *underfitting*, repetimos el experimento con todas las variables y sin parámetro $\lambda$:

```{r}
ridge.mod = glmnet(x, y, alpha=0, lambda=0)
ridge.pred = predict (ridge.mod, s=lambda.min, newx=x.test.boston)
cat("MSE (lambda=0)", mean((ridge.pred - y.test.boston)^2))
```

```{r}
x <- model.matrix(crim~., Boston)[,-1]
train.index <- sample(1:nrow(x), size = nrow(x)*4/5)
x.train.boston <- x[train.index,]
x.test.boston <- x[-train.index,]

cv.wdecay <- cv.glmnet(x.train.boston, y.train.boston, 
                       family = "gaussian", alpha=0) 

lambda.min <- cv.wdecay$lambda.min 
cat("Lambda con menor error: ", lambda.min)
```

```{r}
ridge.mod = glmnet(x, y, alpha=0, lambda=lambda.min)
ridge.pred = predict (ridge.mod, s=lambda.min, newx=x.test.boston)
cat("MSE todos los param:", mean((ridge.pred - y.test.boston)^2))
```

  Los errores obtenidos son mayores, con lo que deducimos que el modelo está bien ajustado y no hay *underfitting*.
  
#### c) Definir una nueva variable con valores -1 y 1 usando el valor de la mediana de la variable *crim* como umbral. Ajustar un modelo SVM que prediga la nueva variable definida. (Usar el paquete e1071 de R). Describir con detalle cada uno de los pasos dados en el aprendizaje del modelo SVM. Comience ajustando un modelo lineal y argumente si considera necesario usar algún núcleo. Valorar los resultados del uso de distintos núcleos.

Comenzamos cargando el paquete $\texttt{e1071}$ y creando el $\texttt{data frame}$ que contenga la etiqueta correspondiente al valor *crim* de cada dato. Comenzamos aplicando SVM directamente, usando un núcleo lineal. Posteriormente veremos la influencia de los parámetros y los ajustaremos haciendo validación cruzada:

```{r, message=FALSE, warning=FALSE}
library(e1071)
```


```{r}
label.crim <- sign(Boston$crim - median(Boston$crim))
Boston.class <- data.frame(x=Boston[-1], y=as.factor(label.crim))
svm.model  <- svm(y~., data = Boston.class, kernel="linear")
```

Una vez obtenido el modelo *SVM*, lo aplicamos sobre los datos y se obtiene la matriz de contingencia, de donde sacamos el error dentro de la muestra:

```{r}
y.svm.pred = predict(svm.model, Boston.class)
confusion.matrix <- table(predict= y.svm.pred, truth= Boston.class$y)
E_in <- (confusion.matrix[2,1]+confusion.matrix[1,2])/sum(confusion.matrix)
print(confusion.matrix)
cat("Ein", E_in*100)
```

Para ajustar el modelo, partimos en primer lugar en datos de entrenamiento y test:

```{r}
x <- model.matrix(y~., Boston.class)[,-1]
y <- label.crim

train.index <- sample(1:nrow(x), size = nrow(x)*4/5)

train.boston <- Boston.class[train.index,]
test.boston <- Boston.class[-train.index,]
```

Realizamos el ajuste sobre la penalización sobre los datos mal clasificados, *cost*:

```{r}
tune.out <- tune(svm, y~., data = train.boston, kernel="linear",
                 ranges=list(cost= c(0.001, 0.01, 0.1, 1, 5, 10, 100))
                 )
best.cost.svm <- tune.out$best.parameters[,1]
cat("Mejor valor del parámetro cost:", best.cost.svm)
```

Observamos la influencia sobre los datos de test:

```{r}
svm.model  <- svm(y~., data = train.boston, kernel="linear")
y.svm.pred = predict(svm.model , test.boston)

confusion.matrix <- table(predict=y.svm.pred, truth=test.boston$y)
E_test.linear <- (confusion.matrix[2,1]+confusion.matrix[1,2])/
  sum(confusion.matrix)
print(confusion.matrix)
cat("E_test para núcleo lineal", E_test.linear*100)
```

Repetimos ahora el experimento para los distintos núcleos:

```{r}
tune.out <- tune(svm, y~., data = train.boston, kernel="polynomial",
                 ranges=list(cost= c(0.001, 0.01, 0.1, 1, 5, 10, 100))
                 )
best.cost.svm <- tune.out$best.parameters[,1]
svm.model  <- svm(y~., data = train.boston, kernel="polynomial")
y.svm.pred = predict(svm.model , test.boston)

confusion.matrix <- table(predict=y.svm.pred, truth=test.boston$y)
E_test.polynomial <- (confusion.matrix[2,1]+confusion.matrix[1,2])/
  sum(confusion.matrix)
print(confusion.matrix)
cat("E_test para núcleo polinómico", E_test.polynomial*100)
```

```{r}
tune.out <- tune(svm, y~., data = train.boston, kernel="radial",
                 ranges=list(cost= c(0.001, 0.01, 0.1, 1, 5, 10, 100))
                 )
best.cost.svm <- tune.out$best.parameters[,1]
svm.model  <- svm(y~., data = train.boston, kernel="radial")
y.svm.pred = predict(svm.model , test.boston)

confusion.matrix <- table(predict=y.svm.pred, truth=test.boston$y)
E_test.radial <- (confusion.matrix[2,1]+confusion.matrix[1,2])/
  sum(confusion.matrix)
print(confusion.matrix)
cat("E_test para núcleo de base radial", E_test.radial*100)
```

```{r}
tune.out <- tune(svm, y~., data = train.boston, kernel="sigmoid",
                 ranges=list(cost= c(0.001, 0.01, 0.1, 1, 5, 10, 100))
                 )
best.cost.svm <- tune.out$best.parameters[,1]
svm.model  <- svm(y~., data = train.boston, kernel="sigmoid")
y.svm.pred = predict(svm.model , test.boston)

confusion.matrix <- table(predict=y.svm.pred, truth=test.boston$y)
E_test.sigmoid <- (confusion.matrix[2,1]+confusion.matrix[1,2])/
  sum(confusion.matrix)
print(confusion.matrix)
cat("E_test para núcleo sigmoidal", E_test.sigmoid*100)
```

  El mejor resultado se da para el núcleo de base radial. Por contra, el peor resultado se da para el núcleo sigmoidal.
  
#### Bonus. Estimar el error de entrenamiento y test por validación cruzada de 5 particiones.


```{r}
detach("package:MASS")
detach("package:glmnet")
detach("package:e1071")
```


# Ejercicio 3

### Usar el conjunto de datos Boston y las librerías randomForest y gbm de R.

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(randomForest)
```

#### 1) Dividir la base de datos en dos conjuntos de entrenamiento (80$\%$) y test (20$\%$).

Comenzamos haciendo la partición de la misma manera que en los apartados previos y reestableciendo la semilla.

```{r}
set.seed(31415926)
data("Boston")
train.boston <- sample(1:nrow(Boston), size = nrow(Boston)*0.8)
```

#### 2) Usando la variable *medv* como salida y el resto como predictoras, ajustar un modelo de regresión usando bagging. Explicar cada uno de los parámetros usados. Calcular el error del test.

```{r}
bag.boston = randomForest(medv~., data=Boston, subset=train.boston,
                          importance = TRUE )
y.pred.bag = predict( bag.boston, newdata=Boston[-train.boston,])
plot(y.pred.bag, Boston[-train.boston,"medv"],
     main="Pred. medv vs real", xlab = "Predicted", ylab = "Medv")
abline(0 ,1)
cat("MSE", mean ((y.pred.bag-Boston[-train.boston,"medv"])^2))
```

#### 3) Ajustar un modelo de regresión usando ``Random Forest''. Obtener una estimación del número de árboles necesario. Justificar el resto de parámetros usados en el ajuste. Calcular el error de test y compararlo con el obtenido con bagging.

```{r}
error.stable <- FALSE
best.ntree <- 10
nTrees <- 10
error.prev <- Inf
error.best <- Inf
  
while( !error.stable && nTrees < 5000){
  mtry <- tuneRF(Boston[train.boston, -14], 
                 Boston[train.boston, "medv"],
                 ntreeTry = nTrees, trace = FALSE, plot = FALSE)
  mtry <- mtry[which.min(mtry[,2]),1]
  bag.boston <- randomForest(medv~., data=Boston, subset=train.boston,
                            mtry=mtry, importance = T, nTrees=nTrees)
  y.pred.rf <- predict( bag.boston, newdata=Boston[-train.boston,])
  error.current <- mean((y.pred.rf - Boston[-train.boston,"medv"])^2)
  
  if(error.best > error.current){
    best.ntree <- nTrees
    error.best <- error.current
  }
  
  if( abs(error.prev - error.current) < 0.01){
    error.stable <- TRUE
  }
  else{
    error.prev <- error.current
    nTrees <- nTrees +10
  }
}
```

Paramos la ejecución cuando llegamos a que el error varía menos de un umbral, obteniendo $280$ como mejor valor para *ntree*. Entonces ajustamos *mtry* para este número de árboles.

```{r}
mtry <- tuneRF(Boston[train.boston, -which(names(Boston) %in% "medv")],
               Boston[train.boston, "medv"], ntreeTry = best.ntree,
               trace = FALSE, plot=FALSE)
mtry <- mtry[which.min(mtry[,2]),1]
bag.boston = randomForest(medv~., data=Boston, subset=train.boston,
                          mtry=mtry, importance = T, nTrees=best.ntree)
y.pred.rf = predict( bag.boston, newdata=Boston[-train.boston,])
cat("MSE:", mean((y.pred.rf - Boston[-train.boston,"medv"])^2))
```

  Vemos que se obtiene un error similar al obtenido con los valores por defecto, y es que estos parámetros ofrecen buenos resultados.
  
#### 4) Ajustar un modelo de regresión usando Boosting (usar *gbm* con $\texttt{distribution = 'gaussian'}$). Calcular el error de test y compararlo con el obtenido con *bagging* y Random Forest.

```{r}
detach("package:randomForest")
library(gbm)
library(caret)
```

  Realizamos en primer lugar *boosting* con los datos de entrenamiento. Probaremos a pasarle como parámetros $5000$ árboles con $4$ niveles de profundidad.

```{r}
boost.boston = gbm(medv~., data=Boston[train.boston,],
                    distribution="gaussian", n.trees=5000,
                    interaction.depth=4)
print(summary(boost.boston))
```

Se observa que las variables más importantes (mucho más que las demás) son *rm* y *lstat* con lo que es de esperar que sean las que determinen los nodos del árbol.

```{r}
y.pred.boost = predict(boost.boston, newdata=Boston[-train.boston,],
                       n.trees = 5000)
cat("MSE:",mean((y.pred.boost - Boston[-train.boston,"medv"])^2))
```


Evaluamos ahora con diferentes parámetros para el número de árboles, la profundidad y el parámetro de disminución. La ejecución no se incluye en el informe porque es muy costosa en tiempo. Los resultados obtenidos son *n.trees*$=4450$, *shrinkage*$=0.01$ y *depth*$=5$.

```{r, eval=FALSE}
train(Boston[train.boston, -which(names(Boston) %in% "medv")],
      Boston[train.boston, "medv"], method = "gbm",
      trControl = trainControl(method = "cv", number=5),
      tuneGrid = expand.grid(interaction.depth = 1:5, 
                             n.trees=seq(50,5000,by=200),
                             shrinkage=seq(0.01,1,by=0.1),
                             n.minobsinnode=1))
```

```{r}
boost.boston = gbm(medv~., data=Boston[train.boston,],
                    distribution="gaussian", n.trees=4450,
                    interaction.depth =5, shrinkage=0.01
                   ) 
y.pred.boost = predict(boost.boston, newdata=Boston[-train.boston,],
                       n.trees = 4450) 
cat("MSE:",mean((y.pred.boost - Boston[-train.boston,"medv"])^2))
```

  Vemos que el error es significativamente menor si ajustamos los parámetros, pasando de 16 a 11, reduciéndose así también el error obtenido con *random forest*
  
```{r}
detach("package:gbm")
detach("package:caret")
```

# Ejercicio 4

### Usar el conjunto de datos OJ que es parte del paquete $\texttt{ILSR}$

```{r}
library(ISLR)
data("OJ")
set.seed(3241592)
```


#### 1) Crear un conjunto de entrenamiento conteniendo una muestra aleatoria de 800 observaciones, y un conjunto de test conteniendo el resto de las observaciones. Ajustar un árbol a los datos de entrenamiento, con *Purchase* como la variable respuesta y las otras variables como predictores (paquete $\texttt{tree}$ de R).

```{r}
library(tree)
```


  Comenzamos tomando un conjunto de entrenamiento de tamaño 800:
  
```{r}
train.OJ <- sample(1:nrow(OJ), size = 800, replace = FALSE)
```


  Ponemos *Purchase* como variable a estudiar en función de las demás y generamos el árbol con la función *tree*
```{r}
tree.OJ <- tree(Purchase~., data=OJ, subset=train.OJ)
```

#### 2) Usar la función summary() para generar un resumen estadístico acerca del árbol y describir los resultados obtenidos: tasa de error de *training*, número de nodos del árbol, etc.

```{r}
summary(tree.OJ)
```

  Vemos que el número de nodos terminales es 8 y que las variables que realmente se usan para la clasificación son *LoyalCH* y *PriceDiff*. El error obtenido en train es del $14.5\%$.
  
#### 3) Crear un dibujo del árbol e interpretar los resultados.

```{r}
plot(tree.OJ)
text(tree.OJ, pretty =0)
```

Se observa cómo realmente el árbol no es minimal, sino que en la rama dada por $LoyalCH<0.5036$, se divide de nuevo por $LoyalCH<0.276142$ y divide otra vez pero da lugar a dos nodos terminales con la misma etiqueta, por lo que se podría ahorrar un nodo. Pasa algo análogo en la rama de $LoyalCH>0.5036$, pues podría diferenciar las categorías cuando está en la rama $LoyalCH<0.76452$ poniendo directamente $PriceDiff<-0.165$.

#### 4) Predecir la respuesta de los datos de test, y generar e interpretar la matriz de confusión de los datos de test. ¿Cuál es la tasa de error del test? ¿Cuál es la precisión del test?

```{r}
tree.OJ.pred = predict(tree.OJ, OJ[-train.OJ,-1],type ="class")
confusion.matrix.OJ <-table(tree.OJ.pred, OJ[-train.OJ,1] )
print(confusion.matrix.OJ)
cat("Precisión", 
    100*sum(diag(confusion.matrix.OJ))/sum(confusion.matrix.OJ))
```

Se observa cómo se han predicho acertadamente 127 datos para la clase *CH* y se han etiquetado como *CH* siendo *MM* 37 datos. En cambio para *MM* se han acertado 84 datos y se han etiquetado como *MM* siendo *CH* 22 datos. La precisión es del $78.1\%$, que significa que hay casi un $22\%$ de error. Probamos ahora con *cv.tree* para ajustar el número óptimo de nodos terminales.

#### 5) Aplicar la función $\texttt{cv.tree()}$ al conjunto de *training* y determinar el tamaño óptimo del árbol. ¿Qué hace $\texttt{cv.tree}$?

Hacemos *CV* sobre *tree* obteniendo los errores y los tamaños con los que se han obtenido, pudiendo elegir aquel con menor error
```{r}
tree.cv.OJ <- cv.tree(tree.OJ, K=5)
size.min.OJ <- tree.cv.OJ$size[which.min(tree.cv.OJ$dev)]
```

```{r}
prune.OJ <- prune.tree(tree.OJ, best=size.min.OJ)
plot(prune.OJ)
text(prune.OJ)
```

```{r}
tree.OJ.pred = predict(prune.OJ, OJ[-train.OJ,-1],type ="class")
confusion.matrix.OJ <-table(tree.OJ.pred, OJ[-train.OJ,1] )
print(confusion.matrix.OJ)
cat("Precisión", 
    100*sum(diag(confusion.matrix.OJ))/sum(confusion.matrix.OJ))
```

  Hemos reducido en un nodo el tamaño del árbol y en cambio hay una pérdida de precisión en torno al $1\%$.
  
#### Bonus 4. Generar un gráfico con el tamaño del árbol en el eje x (número de nodos) y la tasa de error de validación cruzada en el eje y. ¿Qué tamaño de árbol corresponde a la tasa más pequeña de error de clasificación por validación cruzada?

Ya hemos visto que podemos acceder al vector con los errores y tamaños, así que simplemente hay que hacer plot. El mínimo está puesto en rojo:

```{r}
plot(tree.cv.OJ$size, tree.cv.OJ$dev, type="b", ylab = "Error",
     main = "Error según nodos terminales", xlab = "Número de nodos",
     col=(7:1==which.min(tree.cv.OJ$dev))[7:1]+1)
```

```{r}
detach("package:tree")
detach("package:ISLR")
```

